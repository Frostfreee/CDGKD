<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="A project page for 'A Cross-Modal Densely Guided Knowledge Distillation Based on Modality Rebalancing Strategy for Enhanced Unimodal Emotion Recognition'. This paper proposes a framework to leverage a multimodal teacher network to enhance a unimodal student network for emotion recognition.">
  <meta property="og:title" content="A Cross-Modal Densely Guided Knowledge Distillation for Emotion Recognition"/>
  <meta property="og:description" content="This paper proposes a cross-modal knowledge distillation framework that leverages a multimodal teacher network to fuse visual and EEG features and efficiently transfer them to a unimodal student network."/>
  <meta property="og:url" content="YOUR_PROJECT_PAGE_URL"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="A Cross-Modal Densely Guided Knowledge Distillation for Emotion Recognition">
  <meta name="twitter:description" content="This paper proposes a cross-modal knowledge distillation framework that leverages a multimodal teacher network to fuse visual and EEG features and efficiently transfer them to a unimodal student network.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Cross-Modal Knowledge Distillation, Emotion Recognition, Modality Rebalancing, Multimodal Fusion, EEG, Visual Modality">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>A Cross-Modal Densely Guided Knowledge Distillation for Emotion Recognition</title>
  <link rel="icon" type="image/x-icon" href="static/images/image.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Cross-Modal Densely Guided Knowledge Distillation Based on Modality Rebalancing Strategy for Enhanced Unimodal Emotion Recognition</h1>
            <div class="is-size-5 publication-authors">
                <span class="author-block">Shuang Wu<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Heng Liang<sup style="color:#ed4b82;">2</sup>,</span>
                <span class="author-block">Yong Zhang<sup style="color:#ffac33;">3</sup><sup>*</sup>,</span>
                <span class="author-block">Yanlin Chen<sup style="color:#007bff;">4</sup>,</span>
                <span class="author-block">
                  <a href="https://ziyujia.github.io/" target="_blank">Ziyu Jia</a><sup style="color:#9b51e0;">5</sup><sup>*</sup>
                </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>South China University of Technology,</span>
              <span class="author-block"><sup style="color:#ed4b82;">2</sup>The University of Hong Kong,</span>
              <span class="author-block"><sup style="color:#ffac33;">3</sup>Huzhou University,</span>
              <span class="author-block"><sup style="color:#007bff;">4</sup>New York University,</span>
              <span class="author-block"><sup style="color:#9b51e0;">5</sup>Institute of Automation, Chinese Academy of Sciences</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">IJCAI 2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                   <span class="link-block">
                  <a href="LINK_TO_YOUR_PDF" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

                    <span class="link-block">
                    <a href="https://github.com/YOUR_REPO_HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV_PAPER_ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="display: flex; flex-direction: column; align-items: center;">
      <img src="static/images/overall_frame.png" alt="Overall Framework" style="max-width: 80%; height: auto; margin-bottom: 2rem;" />
      <h2 class="subtitle has-text-centered">
        Our proposed framework enhances unimodal (Visual) emotion recognition by distilling knowledge from a multimodal (Visual+EEG) teacher network. We introduce a modality rebalancing strategy to improve the teacher network and a densely guided distillation method to effectively transfer knowledge while minimizing error accumulation.
      </h2>
    </div>
  </div>
</section>
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content" style="font-size: 1.1rem; text-align: justify;">
          <p style="text-align: justify;">
  <b>Multimodal emotion recognition</b> has garnered significant attention for its ability to integrate data from multiple modalities to enhance performance. However, <b>physiological signals</b> like <b>electroencephalogram</b> are more challenging to acquire than <b>visual data</b> due to higher collection costs and complexity. This limits the practical application of multimodal networks. To address this issue, this paper proposes a <b>cross-modal knowledge distillation</b> framework for emotion recognition. The framework aims to leverage the strengths of a <b>multimodal teacher network</b> to enhance the performance of a <b>unimodal student network</b> using only the visual modality as input. Specifically, we design a <b>prototype-based modality rebalancing strategy</b>, which dynamically adjusts the convergence rates of different modalities to mitigate the <b>modality imbalance</b> issue. It enables the teacher network to better integrate multimodal information. Building upon this, we develop a <b>Cross-Modal Densely Guided Knowledge Distillation (CDGKD)</b> method, which effectively transfers knowledge extracted by the multimodal teacher network to the unimodal student network. Our CDGKD uses <b>multi-level teacher assistant networks</b> to bridge the teacher-student gap and employs <b>dense guidance</b> to reduce <b>error accumulation</b> during knowledge transfer. Experimental results demonstrate that the proposed framework outperforms existing methods on two public emotion datasets, providing an effective solution for emotion recognition in <b>modality-constrained scenarios</b>.
</p>


        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item" style="display: flex; flex-direction: column; align-items: center;">
          <img src="static/images/overall_frame.png" alt="The proposed cross-modal knowledge distillation framework" style="max-width: 70%; height: auto; margin: 2rem 0;" />
          <h2 class="subtitle has-text-centered">
            The proposed cross-modal knowledge distillation framework.
          </h2>
        </div>
        <div class="item" style="display: flex; flex-direction: column; align-items: center;">
          <img src="static/images/proto.png" alt="Overview of the multimodal teacher network training process" style="max-width: 70%; height: auto; margin: 2rem 0;" />
          <h2 class="subtitle has-text-centered">
            Overview of the multimodal teacher network training process.
          </h2>
        </div>
        <div class="item" style="display: flex; flex-direction: column; align-items: center;">
          <img src="static/images/DGKD.png" alt="Overview of the CDGKD framework" style="max-width: 70%; height: auto; margin: 2rem 0;" />
          <h2 class="subtitle has-text-centered">
            Overview of the CDGKD framework.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End image carousel -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Methodology</h2>

    <div class="columns is-vcentered" style="margin-top: 2rem;">
      <div class="column is-5">
        <img src="static/images/proto.png" alt="Diagram of the Prototype-Based Modality Rebalancing Strategy" style="border-radius:10px;"/>
        <p class="has-text-centered" style="margin-top: 0.5rem;">Figure 1: The training process of our multimodal teacher network. </p>
      </div>
      <div class="column is-6 is-offset-1">
        <h3 class="title is-4">1. Prototype-Based Modality Rebalancing Strategy</h3>
        <div class="content" style="text-align: justify;">
          <p>
            <strong>Motivation:</strong> When fusing data from different sources like video and EEG, networks often face "modality imbalance," where one modality dominates and suppresses the other. This limits the overall performance of the fusion network. 
          </p>
          <p>
            <strong>Our Approach:</strong> To solve this, we introduce a modality rebalancing strategy using prototype loss. A "prototype" serves as a central feature vector for each emotion class. By measuring how closely each modality's features cluster around these prototypes, we can estimate its convergence rate. Our method then dynamically adjusts the loss weights to boost the slower-learning modality, ensuring it contributes effectively. This allows the teacher network to learn a more balanced and powerful multimodal representation for later distillation.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-vcentered" style="margin-top: 3rem;">
      <div class="column is-6">
        <h3 class="title is-4">2. Cross-Modal Densely Guided Knowledge Distillation (CDGKD)</h3>
        <div class="content" style="text-align: justify;">
          <p>
            <strong>Motivation:</strong> Transferring knowledge from a complex multimodal "teacher" to a simple unimodal "student" is difficult due to the large structural gap between them. Furthermore, traditional step-by-step distillation can suffer from "error accumulation," where mistakes from one stage are amplified in the next. 
          </p>
          <p>
            <strong>Our Approach:</strong> We propose CDGKD, which uses multiple Teacher Assistant (TA) networks to bridge this gap. The core innovation is dense guidance: the student learns not just from the immediately preceding TA, but from a combined set of all higher-level networks, including the original teacher. We also employ a stochastic learning strategy, which randomly selects knowledge sources during training. This prevents the network from relying on a single knowledge path, reduces the risk of overfitting, and effectively mitigates error accumulation. 
          </p>
        </div>
      </div>
      <div class="column is-5 is-offset-1">
        <img src="static/images/DGKD.png" alt="Overview of the Cross-Modal Densely Guided Knowledge Distillation (CDGKD) framework" style="border-radius:10px;"/>
        <p class="has-text-centered" style="margin-top: 0.5rem;">Figure 2: The CDGKD framework. </p>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>
    <div class="columns is-centered">
      <div class="column is-10">

        <h3 class="title is-4 has-text-centered">Student Network Performance (CDGKD)</h3>
        <!-- <p class="has-text-centered content">
          As shown in Table 1, our proposed <strong>CDGKD</strong> method significantly outperforms existing knowledge distillation baselines on both the DEAP and MAHNOB-HCI datasets. Unlike methods that struggle with the structural gap or suffer from error accumulation, our approach effectively integrates knowledge from all higher-level networks, providing diverse guidance that leads to superior classification performance for the final unimodal student network.
        </p> -->
        <img src="static/images/table1.png" alt="Table 1: Comparison of student network performance across knowledge distillation baselines" style="max-width: 80%; margin: 1rem auto; display: block; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
        <p class="has-text-centered" style="margin-top: 0.5rem;">Table 1: Comparison of student network performance across knowledge distillation baselines. </p>
        <h3 class="title is-4 has-text-centered" style="margin-top: 4rem;">Teacher Network Performance (Prototype Loss)</h3>
        <!-- <p class="has-text-centered content">
          Table 2 demonstrates that our multimodal teacher network, enhanced with the <strong>Prototype-Based Modality Rebalancing Strategy</strong>, also achieves state-of-the-art performance. By explicitly balancing the convergence rates of the visual and EEG modalities, our teacher network avoids the common issue of modality imbalance and generates higher-quality multimodal knowledge to be distilled. This provides a stronger foundation for the student network's learning process.
        </p> -->
        <img src="static/images/table2.png" alt="Table 2: Comparison of the teacher network performance with baselines" style="max-width: 80%; margin: 1rem auto; display: block; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
        <p class="has-text-centered" style="margin-top: 0.5rem;">Table 2: Comparison of the teacher network performance with baselines. </p>
      </div>
    </div>
  </div>
</section>


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/YOUR_YOUTUBE_ID" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Video Presentation</h2>

    <div class="tabs is-centered is-boxed is-medium">
      <ul>
        <li class="is-active" data-tab="english-video-content">
          <a>
            <span class="icon is-small"><i class="fas fa-globe-americas" aria-hidden="true"></i></span>
            <span>English</span>
          </a>
        </li>
        <li data-tab="chinese-video-content">
          <a>
            <span class="icon is-small"><i class="fas fa-language" aria-hidden="true"></i></span>
            <span>中文讲解</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="has-text-centered">
      <div id="english-video-content" class="tab-content">
        <video width="100%" controls preload="auto">
          <source src="static/videos/Eng_video.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div id="chinese-video-content" class="tab-content" style="display: none;">
        <video width="100%" controls preload="auto">
          <source src="static/videos/Ch_video.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

    <div class="buttons is-centered" style="margin-top: 1.5rem;">
      <a href="static/ppts/IJCAI25_Eng.pptx" class="button is-link is-rounded">
        <span class="icon">
          <i class="fas fa-file-powerpoint"></i>
        </span>
        <span>Download English Slides (.pptx)</span>
      </a>
      <a href="static/ppts/IJCAI25_Ch.pptx" class="button is-success is-rounded">
        <span class="icon">
          <i class="fas fa-file-alt"></i>
        </span>
        <span>下载中文版PPT (.pptx)</span>
      </a>
    </div>

  </div>
</section>

<script>
  document.addEventListener('DOMContentLoaded', () => {
    // Select all tab buttons within the '.tabs' container
    const tabs = document.querySelectorAll('.tabs li');
    // Select all content panes
    const tabContents = document.querySelectorAll('.tab-content');

    tabs.forEach(tab => {
      tab.addEventListener('click', () => {
        const targetId = tab.dataset.tab;
        const targetContent = document.getElementById(targetId);

        // Deactivate all tabs and hide all content panes
        tabs.forEach(t => t.classList.remove('is-active'));
        tabContents.forEach(content => {
          content.style.display = 'none';
          // Pause the video when its tab is hidden
          const video = content.querySelector('video');
          if (video) {
            video.pause();
          }
        });

        // Activate the clicked tab and show the corresponding content
        tab.classList.add('is-active');
        if (targetContent) {
          targetContent.style.display = 'block';
        }
      });
    });
  });
</script>


<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Qualitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Poster</h2>
      <iframe  src="static/pdfs/IJCAI25_poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{wu2025cross,
  title     = {A Cross-Modal Densely Guided Knowledge Distillation Based on Modality Rebalancing Strategy for Enhanced Unimodal Emotion Recognition},
  author    = {Wu, Shuang and Liang, Heng and Zhang, Yong and Chen, Yanlin and Jia, Ziyu},
  booktitle = {Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI-25},
  year      = {2025}
}</code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>